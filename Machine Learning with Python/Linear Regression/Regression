regression is the process of predicting a continuous value
y must be continous

x1,x2,x3,.... can be discrete or categorical or continous

Linear regression is the approximation of a linear model used to describe the relationship between two or more variables.

y = mx + b
linear regessiob estimates m (slope) and b (intercept) coefs

residual error = actual value - predicted 
the error is the distance from the data point to the fitted regression line.

(MSE) The mean of all residual errors shows how poorly the line fits with the whole data set 
The objective of linear regression, is to minimize this MSE equation (best parameters for m, b)

b is bias coefficient

Pros of Linear Regression
fast, does not require tuning of paremeters, eay to understand and highly interpretable



training and testing on the same dataset produces a high training accuracy
Training accuracy is the percentage of correct predictions that the model makes when using the test dataset
high training accuracy isn't necessarily a good thing. For instance, having a high training accuracy may result in an over-fit the data

Out-of-sample accuracy is the percentage of correct predictions that the model makes on data that the model has not been trained on
Doing a train and test on the same dataset will most likely have low out-of-sample accuracy due to the likelihood of being over-fit.

training/test split better out of samply accuracy vs test on portion of train
The variation of this causes train/test split to have a better out-of-sample prediction than training and testing on the same dataset, but it still has some problems due to this dependency.

training/test highly depenant on training data (fixes high varaition)

k-fold cross validation fixes this
each fold is used for testing
average each fold

regresion accuracy = actual - predicted

The difference between the data points and the trend line generated by the algorithm  is the error

MAE
MSE
RAE = Σ|actual - predicted| / Σ|actual - mean|
RSS = Σ(actual - predicted)^2
RMSE
RSE


Theta is also called the parameters or weight vector of the regression equation [see mult regression pic]
x is the feature set

The first element of the feature set would be set to one, because it turns that theta zero into the intercept or biased parameter when the vector is multiplied by the parameter vector

p1 = line, high dimensions = hyperplane

Ordinary least squares tries to estimate the values of the coefficients by minimizing the mean square error. This approach uses the data as a matrix and uses linear algebra operations to estimate the optimal values for the theta. The problem with this technique is the time complexity of calculating matrix operations as it can take a very long time to finish

The second option is to use an optimization algorithm to find the best parameters. That is, you can use a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data. For example, you can use gradient descent which starts optimization with random values for each coefficient, then calculates the errors and tries to minimize it through y's changing of the coefficients in multiple iterations. Gradient descent is a proper approach if you have a large data set. Please understand however, that there are other approaches to estimate the parameters of the multiple linear regression that you can explore on your own

adding too many independent variables without any theoretical justification may result in an overfit model

categorical independent variables can be incorporated into a regression model by converting them into numerical variables

remember that multiple linear regression is a specific type of linear regression. So, there needs to be a linear relationship between the dependent variable and each of your independent variables. There are a number of ways to check for linear relationship. For example, you can use scatter plots and then visually checked for linearity. If the relationship displayed in your scatter plot is not linear, then you need to use non-linear regression
